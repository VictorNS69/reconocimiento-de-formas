{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfaNEPeRXCe8"
   },
   "source": [
    "# Práctica 4 Reconocimiento de Formas: Clasificador Bayesiano Paramétrico\n",
    "\n",
    "* **Alumno 1**: Javier Barragán Haro\n",
    "* **Alumno 2**: Victor Nieves Sanchez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsWqPK6l2flD",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import ShrunkCovariance\n",
    "from sklearn import preprocessing\n",
    "from abc import abstractmethod\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self,X,y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self,X):\n",
    "        pass\n",
    "\n",
    "class ClassifBayesianoParametrico(Classifier):\n",
    "    def __init__(self, share_covs=False, shrinkage=0.0):\n",
    "        \"\"\"Constructor de la clase\n",
    "        share_covs: Indica si la matriz de covarianzas va a ser compartida entre las distintas clases.\n",
    "        shrinkage: Parámetro que determina la diagonalidad de la matriz de covarianzas. Ver sklearn.covariance.ShrunkCovariance\n",
    "        \"\"\"\n",
    "        assert 0 <= shrinkage <= 1\n",
    "        self.labels = None\n",
    "        self.ln_apriories = None\n",
    "        self.means = None\n",
    "        self.ln_determinants = None\n",
    "        self.inv_covs = None\n",
    "        self.share_covs = share_covs\n",
    "        self.shrinkage = shrinkage\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena el clasificador\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        y: vector de etiquetas, tantos elementos como filas en X\n",
    "        retorna objeto clasificador\"\"\"\n",
    "        assert X.ndim == 2 and X.shape[0] == len(y)\n",
    "        \n",
    "        self.labels = y\n",
    "\n",
    "        # Aseguramos que las etiquetas son numeros tal que: [0, 1, ..., N]\n",
    "        y = pd.factorize(y)[0]\n",
    "        \n",
    "        # Preprocesamos los datos de entrada\n",
    "        X = self.scaler.fit_transform(X)    \n",
    "        \n",
    "        # Contar cuantos ejemplos hay de cada etiqueta\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        self.ln_apriories = np.array([np.log((counts[i]/np.sum(counts))) for i in unique])\n",
    "        \n",
    "        # Calcular para los ejemplos de cada clase, la media de cada una de sus características (centroide)\n",
    "        self.means = np.array([np.mean(X[y==i], axis=0) for i in unique])\n",
    "        \n",
    "        if self.share_covs:\n",
    "            # Restamos al dato de cada clase su centroide\n",
    "            new_X = np.array([])\n",
    "            #for i in unique:\n",
    "            new_X = [np.append(new_X.reshape(-1, X.shape[1]), X[y==i] - self.means[i], axis=0) for i in unique]\n",
    "\n",
    "            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n",
    "            cov = ShrunkCovariance(shrinkage=self.shrinkage).fit(new_X).covariance_\n",
    "\n",
    "            # La reproducimos tantas veces como número de clases\n",
    "            covs = np.tile(cov, (len(unique), 1, 1))\n",
    "\n",
    "        else:\n",
    "            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n",
    "            covs = np.array([ShrunkCovariance(shrinkage=self.shrinkage).fit(X[y==i]).covariance_ for i in unique])\n",
    "            \n",
    "        # Para cada una de las clases, calcular el logaritmo neperiano de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n",
    "        self.ln_determinants = np.log(list(map(np.linalg.det, covs)))\n",
    "        # Para cada una de las clases, calcular la inversa de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n",
    "        self.inv_covs = np.array(list(map(np.linalg.pinv,covs)))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas.\n",
    "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
    "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
    "        \n",
    "        assert self.means is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n",
    "        assert X.ndim == 2 and X.shape[1] == self.means.shape[1]\n",
    "\n",
    "        # Preprocesamos nuestros datos\n",
    "        X = self.scaler.fit_transform(X)        \n",
    "\n",
    "        # Resta la media de cada clase a cada ejemplo en X\n",
    "        new_X = self.means[:,np.newaxis,:] - X \n",
    "        \n",
    "        # Calcula el logaritmo de la función de decisión gausiana\n",
    "        # Transparencias de Métodos paramétricos de clasificación: página 14:\n",
    "        # -(1/2)ln|Sigma_i| - (1/2)*(x- mu_i)^T Sigma_i^-1 (x- mu_i) + lnP(alpha_i)\n",
    "        return -0.5 * self.ln_determinants[:, np.newaxis] \\\n",
    "            - 0.5 * np.array([np.sum((new_X[i] @ self.inv_covs[i]) \n",
    "                                     * new_X[i], axis=1) for i in np.unique(y)]) \\\n",
    "            + self.ln_apriories[:, np.newaxis]\n",
    "    \n",
    "    def pred_label(self, X):\n",
    "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
    "        return np.argmax(X, axis = 0)\n",
    "    \n",
    "    def num_aciertos(self, X): \n",
    "        \"\"\"Cuenta el numero de aciertos del clasificador para un conjunto de datos X.\n",
    "        X: matriz de datos a clasificar\"\"\"\n",
    "        same_values = []\n",
    "        [same_values.append(X[i] == self.labels[i]) for i in range(0, len(self.labels))]\n",
    "        number = same_values.count(True)\n",
    "\n",
    "        return number, (number / len(X)) * 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0U0Awu0X01F"
   },
   "source": [
    "## Iris Dataset\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers: 147 / 150\n",
      "Success rate: 98.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "# print(\"X: \\n\" + str(X))\n",
    "y = dataset.target\n",
    "\n",
    "# 2. Entrenar el clasificador\n",
    "ClassifBayesP = ClassifBayesianoParametrico()\n",
    "ClassifBayesP.fit(X,y)\n",
    "\n",
    "# 3. Predecir empleando la base de datos de entrenamiento (X)\n",
    "predict_matrix = ClassifBayesP.predict(X)\n",
    "\n",
    "# 4. Evaluar el clasificador calculando el porcentaje de datos correctamente clasificados\n",
    "labels_matrix = ClassifBayesP.pred_label(predict_matrix)\n",
    "\n",
    "correct = ClassifBayesP.num_aciertos(labels_matrix)\n",
    "print(\"Correct answers:\", correct[0], \"/\", len(y))\n",
    "print(\"Success rate:\", correct[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BsniqwE-9fp-"
   },
   "source": [
    "## Wine dataset\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Wine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dbakRtC79fqA",
    "outputId": "f68024cd-71d7-4e9d-e5aa-78b5cd73c22e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers: 177 / 178\n",
      "Success rate: 99.43820224719101\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "from sklearn.datasets import load_wine\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "# print(\"X: \\n\" + str(X))\n",
    "y = dataset.target\n",
    "# print(\"y: \\n\" + str(y))\n",
    "\n",
    "# 2. Entrenar el clasificador\n",
    "ClassifBayesP = ClassifBayesianoParametrico()\n",
    "ClassifBayesP.fit(X,y)\n",
    "\n",
    "# 3. Predecir empleando la base de datos de entrenamiento (X)\n",
    "predict_matrix = ClassifBayesP.predict(X)\n",
    "\n",
    "# 4. Evaluar el clasificador calculando el porcentaje de datos correctamente clasificados\n",
    "labels_matrix = ClassifBayesP.pred_label(predict_matrix)\n",
    "\n",
    "correct = ClassifBayesP.num_aciertos(labels_matrix)\n",
    "print(\"Correct answers:\", correct[0], \"/\", len(y))\n",
    "print(\"Success rate:\", correct[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7kPXX70NobZ"
   },
   "source": [
    "## Breast cancer dataset\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Breast cancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "_54KE327_QEa",
    "outputId": "b43bf02b-fd87-4258-dab9-89d07b2a516a",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers: 555 / 569\n",
      "Success rate: 97.53954305799648\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "# print(\"X: \\n\" + str(X))\n",
    "y = dataset.target\n",
    "# print(\"y: \\n\" + str(y))\n",
    "\n",
    "# 2. Entrenar el clasificador\n",
    "ClassifBayesP = ClassifBayesianoParametrico()\n",
    "ClassifBayesP.fit(X,y)\n",
    "\n",
    "# 3. Predecir empleando la base de datos de entrenamiento (X)\n",
    "predict_matrix = ClassifBayesP.predict(X)\n",
    "\n",
    "# 4. Evaluar el clasificador calculando el porcentaje de datos correctamente clasificados\n",
    "labels_matrix = ClassifBayesP.pred_label(predict_matrix)\n",
    "\n",
    "correct = ClassifBayesP.num_aciertos(labels_matrix)\n",
    "print(\"Correct answers:\", correct[0], \"/\", len(y))\n",
    "print(\"Success rate:\", correct[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFLkiTYnNobf"
   },
   "source": [
    "## MNIST Database (Modified National Institute of Standards and Technology database)\n",
    "MNIST es una base de datos de texto manuscrito, que se usa de forma clásica para entrenar sistemas de procesado de imágenes.\n",
    "\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqWv7YjfNobj",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers: 52287 / 60000\n",
      "Success rate: 87.145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Cargamos MNIST desde internet ( https://www.openml.org/d/554 )\n",
    "all_X, all_y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X = all_X[:60000]\n",
    "y = all_y[:60000]\n",
    "y = pd.factorize(y)[0]\n",
    "\n",
    "# print(\"X: \\n\" + str(X))\n",
    "# print(\"y: \\n\" + str(y))\n",
    "\n",
    "# 2. Entrenar el clasificador\n",
    "ClassifBayesP = ClassifBayesianoParametrico(share_covs=True, shrinkage=0.3)\n",
    "ClassifBayesP.fit(X,y)\n",
    "\n",
    "# 3. Predecir empleando la base de datos de entrenamiento (X)\n",
    "predict_matrix = ClassifBayesP.predict(X)\n",
    "\n",
    "# 4. Evaluar el clasificador calculando el porcentaje de datos correctamente clasificados\n",
    "labels_matrix = ClassifBayesP.pred_label(predict_matrix)\n",
    "\n",
    "correct = ClassifBayesP.num_aciertos(labels_matrix)\n",
    "print(\"Correct answers:\", correct[0], \"/\", len(y))\n",
    "print(\"Success rate:\", correct[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLQthMw5Nobo"
   },
   "source": [
    "## Isolet Dataset (Isolated Letter Speech Recognition)\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Isolet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFj7GFS4Nobp",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers: 7489 / 7797\n",
      "Success rate: 96.04976272925484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "X, y = fetch_openml('isolet', version=1, return_X_y=True, cache=True)\n",
    "y = pd.factorize(y)[0]\n",
    "# print(\"X: \\n\" + str(X))\n",
    "# print(\"y: \\n\" + str(y))\n",
    "\n",
    "# 2. Entrenar el clasificador\n",
    "ClassifBayesP = ClassifBayesianoParametrico(share_covs=True, shrinkage=0.26) \n",
    "ClassifBayesP.fit(X,y)\n",
    "\n",
    "# 3. Predecir empleando la base de datos de entrenamiento (X)\n",
    "predict_matrix = ClassifBayesP.predict(X)\n",
    "\n",
    "# 4. Evaluar el clasificador calculando el porcentaje de datos correctamente clasificados\n",
    "labels_matrix = ClassifBayesP.pred_label(predict_matrix)\n",
    "\n",
    "correct = ClassifBayesP.num_aciertos(labels_matrix)\n",
    "print(\"Correct answers:\", correct[0], \"/\", len(y))\n",
    "print(\"Success rate:\", correct[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_kQ7OJd9fqD"
   },
   "source": [
    "Resultados de los tres experimentos:\n",
    "\n",
    "| Base de datos | Número de aciertos | Porcentaje de aciertos |\n",
    "| --- | --- | --- |\n",
    "| Iris   |  147 / 150|98.00|\n",
    "| Wine   | 177 / 178|99.44 | \n",
    "| Cancer | 555 / 569|97.54 |\n",
    "| MNIST  | 52287 / 60000|87.15 |\n",
    "| Isolet | 7489 / 7797 |  96.05 |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Solucion4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4+"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
