{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5: Reconocimiento de Formas: Evaluación de rendimiento\n",
    "\n",
    "* **Alumno 1**: Víctor Nieves Sánchez\n",
    "* **Alumno 2**: Javier Barragán Haro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador de la distancia euclídea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import ShrunkCovariance\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import preprocessing\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "class ClassifEuclid(Classifier):\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor de la clase\n",
    "        labels: lista de etiquetas de esta clase\"\"\"\n",
    "        self.centroids = None\n",
    "        self.distancias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena el clasificador\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        y: vector de etiquetas, tantos elementos como filas en X\n",
    "        retorna objeto clasificador\"\"\"\n",
    "        assert X.ndim == 2 and X.shape[0] == len(y)\n",
    "        self.distancias = np.array(np.unique(y))\n",
    "        # Entrena el clasificador\n",
    "        self.centroids = np.array([np.mean(X[y==i], axis=0) for i in np.unique(y)])\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas.\n",
    "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
    "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
    "        assert self.centroids is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n",
    "        assert X.ndim == 2 and X.shape[1] == self.centroids.shape[1]\n",
    "        return np.linalg.norm(self.centroids - X[:, np.newaxis], axis=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
    "        # Calcula y devuelve la clase más probable\n",
    "        return self.distancias[np.argmin(self.decision_function(X), axis=1)]\n",
    "\n",
    "    def  num_aciertos(self,X,y):\n",
    "        estimation = self.predict(X)\n",
    "        aciertos = (estimation == y).sum()\n",
    "        return aciertos, aciertos/len(y) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador Bayesiano Paramétrico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifBayesianoParametrico(Classifier):\n",
    "    def __init__(self, share_covs=False, shrinkage=0.0):\n",
    "        \"\"\"Constructor de la clase\n",
    "        labels: lista de etiquetas de esta clase\"\"\"\n",
    "        assert 0 <= shrinkage <= 1\n",
    "        self.labels = None\n",
    "        self.ln_apriories = None\n",
    "        self.means = None\n",
    "        self.ln_determinants = None\n",
    "        self.inv_covs = None\n",
    "        self.share_covs = share_covs\n",
    "        self.shrinkage = shrinkage\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena el clasificador\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        y: vector de etiquetas, tantos elementos como filas en X\n",
    "        retorna objeto clasificador\"\"\"\n",
    "        assert X.ndim == 2 and X.shape[0] == len(y)\n",
    "        assert np.max(y) < len(np.unique(y)), \"Error: Las etiquetas deben ser enteros entre 0 y el número de clases.\\n\" \\\n",
    "                                              \"Puedes hacer esto empleando:\\ny = pd.factorize(y)[0]\"\n",
    "        # Entrena el clasificador\n",
    "\n",
    "        # Preprocesamos los datos de entrada\n",
    "        X = self.scaler.fit_transform(X)       \n",
    "\n",
    "        # Contar cuantos ejemplos hay de cada etiqueta\n",
    "        self.labels, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        # Usando el contador de ejemplos de cada etiqueta, calcular el logaritmo neperiano de las probabilidades a-priori\n",
    "        self.ln_apriories = np.array([np.log((counts[i]/np.sum(counts))) for i in self.labels])\n",
    "\n",
    "        # Calcular para los ejemplos de cada clase, la media de cada una de sus características (centroide)\n",
    "        self.means = np.array([np.mean(X[y==i], axis=0) for i in self.labels])\n",
    "        \n",
    "        if self.share_covs:\n",
    "            # Restamos al dato de cada clase su centroide\n",
    "            X_mean = np.array([])\n",
    "            for i in self.labels:\n",
    "                X_mean = np.append(X_mean.reshape(-1, X.shape[1]), X[y==i] - self.means[i], axis=0)\n",
    "\n",
    "            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n",
    "            cov = ShrunkCovariance(shrinkage=self.shrinkage).fit(X_mean).covariance_\n",
    "\n",
    "            # La reproducimos tantas veces como número de clases\n",
    "            covs = np.tile(cov, (len(self.labels), 1, 1))\n",
    "         \n",
    "        else:\n",
    "            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n",
    "            covs = np.array([ShrunkCovariance(shrinkage=self.shrinkage).fit(X[y==i]).covariance_ for i in self.labels])\n",
    "\n",
    "        # Para cada una de las clases, calcular el logaritmo neperiano de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n",
    "        self.ln_determinants = np.log(list(map(lambda x: np.linalg.det(x), covs)))\n",
    "\n",
    "        # Para cada una de las clases, calcular la inversa de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n",
    "        self.inv_covs = np.array(list(map(lambda x: np.linalg.pinv(x), covs)))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas.\n",
    "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
    "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
    "        assert self.means is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n",
    "        assert X.ndim == 2 and X.shape[1] == self.means.shape[1]\n",
    "        \n",
    "        # Calcula y devuelve la probabilidad de cada clase\n",
    "        \n",
    "        # Preprocesamos nuestros datos\n",
    "        X = self.scaler.fit_transform(X)        \n",
    "\n",
    "        # Resta la media de cada clase a cada ejemplo en X  \n",
    "        X_mean0 = np.array(X[:, np.newaxis] - self.means)\n",
    "\n",
    "        # Calcula el logaritmo de la función de decisión gausiana\n",
    "        # -(1/2)ln|Sigma_i| - (1/2)*(x- mu_i)^T Sigma_i^-1 (x- mu_i) + lnP(alpha_i)\n",
    "        grado_de_pertenencia = np.array(list(map(lambda x: -(1/2)*(self.ln_determinants \n",
    "                   + np.matmul(np.matmul(x.reshape(len(self.labels),1,X.shape[1]),self.inv_covs), \n",
    "                   x.reshape(len(self.labels),1,X.shape[1]).transpose(0,2,1)).reshape(len(self.labels)))\n",
    "                   + self.ln_apriories, X_mean0)))\n",
    "\n",
    "        return grado_de_pertenencia\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
    "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
    "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
    "        # Calcula y devuelve la clase más probable\n",
    "        return self.labels[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "    def  num_aciertos(self,X,y):\n",
    "        estimation = self.predict(X)\n",
    "        aciertos = (estimation == y).sum()\n",
    "        return aciertos, aciertos/len(y) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_shrinkage_clf(X, y, k, shrinkages, share_covs):\n",
    "    \"\"\"\n",
    "    Busca el clasificador bayesiano regularizado con el mejor shrinkage. \n",
    "    :param X: Ejemplos de la dase de datos\n",
    "    :param y: Etiquetas de los ejemplos\n",
    "    :param k: Número de divisiones en la validación cruzada (k-fold)\n",
    "    :param shrinkages: Lista de posibles shrinkages que conforman la rejilla de búsqueda\n",
    "    :param share_covs: Lista de posibles valores para share_covs que conforman la rejilla de búsqueda\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    cbp = ClassifBayesianoParametrico()\n",
    "    params = {'shrinkage': shrinkages, 'share_covs': share_covs}\n",
    "    clf = GridSearchCV(cbp, params, n_jobs=-2, scoring='accuracy', cv=k).fit(X, y)\n",
    "    best_clf = clf.best_estimator_\n",
    "    # print(\"Srinkage scores: \", clf.cv_results_['mean_test_score'])\n",
    "    result_score_mean = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "    result_score_std = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "    print(\"\\tSelected shrinkage = {}, share_covs = {}\\n\" \\\n",
    "          \"\\tAccuracy: {:.3f} (+/- {:.3f})\\n\".format(best_clf.shrinkage,\n",
    "                                                   best_clf.share_covs,\n",
    "                                                   result_score_mean,\n",
    "                                                   result_score_std))\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Clasificador Euclídeo \u001b[0m \n",
      "\n",
      "Accuracy: 0.99 (+/- 0.03) \n",
      "\n",
      "Matriz de confusión con clasificador de la distancia euclidea: \n",
      " [[50  0  0]\n",
      " [ 0 49  1]\n",
      " [ 0  0 50]] \n",
      "\n",
      "\u001b[1m Clasificador Estadístico Paramétrico \u001b[0m \n",
      "\n",
      "\tSelected shrinkage = 0.1, share_covs = True\n",
      "\tAccuracy: 0.953 (+/- 0.062)\n",
      "\n",
      "Accuracy: 0.95 (+/- 0.12) \n",
      "\n",
      "Matriz de confusión con clasificador bayesiano parametrico: \n",
      " [[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  1 49]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from time import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = pd.factorize(dataset.target)[0]\n",
    "\n",
    "\n",
    "# 2. Baraja los datos para hacer validación cruzada\n",
    "X, y = sklearn.utils.shuffle(X, y)\n",
    "\n",
    "# Evaluar el clasificador de la distancia eucídea usando cross validation (k-fold=5)\n",
    "k = 5\n",
    "print(\"\\033[1m\", \"Clasificador Euclídeo\", \"\\033[0m\", \"\\n\")\n",
    "cde = SVC(kernel='linear', C=1).fit(X,y)\n",
    "scoresCde = cross_val_score(cde, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCde.mean(), scoresCde.std() * 2), \"\\n\")\n",
    "mConfusionCde = confusion_matrix(y, cde.predict(X) , labels=np.unique(y))\n",
    "print(\"Matriz de confusión con clasificador de la distancia euclidea: \\n\", mConfusionCde, \"\\n\")\n",
    "\n",
    "# Haz una selección de modelos para buscar shrinkage del clasificador \n",
    "# estadístico paramétrico que obtiene el mejor accuracy. Usa para ello cross validation (k-fold=5)\n",
    "print(\"\\033[1m\", \"Clasificador Estadístico Paramétrico\", \"\\033[0m\", \"\\n\")\n",
    "\n",
    "clf = best_shrinkage_clf(X, y, k, [1.0, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1, 0.0], [True, False])\n",
    "cbp = ClassifBayesianoParametrico(clf.share_covs, clf.shrinkage).fit(X,y)\n",
    "scoresCbp = cross_val_score(cbp, X, y, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCbp.mean(), scoresCbp.std() * 2), \"\\n\")\n",
    "\n",
    "mConfusionCbp = confusion_matrix(y, cbp.predict(X) , labels=np.unique(y))\n",
    "print(\"Matriz de confusión con clasificador bayesiano parametrico: \\n\", mConfusionCbp, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine dataset\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Wine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Clasificador Euclídeo \u001b[0m \n",
      "\n",
      "Accuracy: 0.97 (+/- 0.05) \n",
      "\n",
      "Matriz de confusión con clasificador de la distancia euclidea: \n",
      " [[59  0  0]\n",
      " [ 0 70  1]\n",
      " [ 0  0 48]] \n",
      "\n",
      "\u001b[1m Clasificador Estadístico Paramétrico \u001b[0m \n",
      "\n",
      "\tSelected shrinkage = 0.4, share_covs = False\n",
      "\tAccuracy: 0.994 (+/- 0.011)\n",
      "\n",
      "Accuracy: 0.99 (+/- 0.02) \n",
      "\n",
      "Matriz de confusión con clasificador bayesiano parametrico: \n",
      " [[59  0  0]\n",
      " [ 0 70  1]\n",
      " [ 0  0 48]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from time import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "from sklearn.datasets import load_wine\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "y = pd.factorize(dataset.target)[0]\n",
    "\n",
    "# 2. Baraja los datos para hacer validación cruzada\n",
    "X, y = sklearn.utils.shuffle(X, y)\n",
    "\n",
    "# Evaluar el clasificador de la distancia eucídea usando cross validation (k-fold=5)\n",
    "k = 5\n",
    "print(\"\\033[1m\", \"Clasificador Euclídeo\", \"\\033[0m\", \"\\n\")\n",
    "cde = SVC(kernel='linear', C=1).fit(X,y)\n",
    "scoresCde = cross_val_score(cde, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCde.mean(), scoresCde.std() * 2), \"\\n\")\n",
    "mConfusionCde = confusion_matrix(y, cde.predict(X) , labels=np.unique(y))\n",
    "print(\"Matriz de confusión con clasificador de la distancia euclidea: \\n\", mConfusionCde, \"\\n\")\n",
    "\n",
    "# Haz una selección de modelos para buscar shrinkage del clasificador \n",
    "# estadístico paramétrico que obtiene el mejor accuracy. Usa para ello cross validation (k-fold=5)\n",
    "print(\"\\033[1m\", \"Clasificador Estadístico Paramétrico\", \"\\033[0m\", \"\\n\")\n",
    "\n",
    "clf = best_shrinkage_clf(X, y, k, [1.0, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1, 0.0], [True, False])\n",
    "cbp = ClassifBayesianoParametrico(clf.share_covs, clf.shrinkage).fit(X,y)\n",
    "scoresCbp = cross_val_score(cbp, X, y, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCbp.mean(), scoresCbp.std() * 2), \"\\n\")\n",
    "mConfusionCbp = confusion_matrix(y, cbp.predict(X) , labels=np.unique(y))\n",
    "print(\"Matriz de confusión con clasificador bayesiano parametrico: \\n\", mConfusionCbp, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Breast cancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Clasificador Euclídeo \u001b[0m \n",
      "\n",
      "Accuracy: 0.95 (+/- 0.01) \n",
      "\n",
      "Matriz de confusión con clasificador de la distancia euclidea: \n",
      " [[201  11]\n",
      " [  8 349]] \n",
      "\n",
      "\u001b[1m Clasificador Estadístico Paramétrico \u001b[0m \n",
      "\n",
      "\tSelected shrinkage = 0.2, share_covs = True\n",
      "\tAccuracy: 0.958 (+/- 0.017)\n",
      "\n",
      "Accuracy: 0.96 (+/- 0.03) \n",
      "\n",
      "Matriz de confusión con clasificador bayesiano parametrico: \n",
      " [[191  21]\n",
      " [  1 356]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from time import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = pd.factorize(dataset.target)[0]\n",
    "\n",
    "# 2. Baraja los datos para hacer validación cruzada\n",
    "X, y = sklearn.utils.shuffle(X, y)\n",
    "\n",
    "# Evaluar el clasificador de la distancia eucídea usando cross validation (k-fold=5)\n",
    "k = 5\n",
    "print(\"\\033[1m\", \"Clasificador Euclídeo\", \"\\033[0m\", \"\\n\")\n",
    "cde = SVC(kernel='linear', C=1).fit(X,y)\n",
    "scoresCde = cross_val_score(cde, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCde.mean(), scoresCde.std() * 2), \"\\n\")\n",
    "mConfusionCde = confusion_matrix(y, cde.predict(X) , labels=np.unique(y))\n",
    "print(\"Matriz de confusión con clasificador de la distancia euclidea: \\n\", mConfusionCde, \"\\n\")\n",
    "\n",
    "# Haz una selección de modelos para buscar shrinkage del clasificador \n",
    "# estadístico paramétrico que obtiene el mejor accuracy. Usa para ello cross validation (k-fold=5)\n",
    "print(\"\\033[1m\", \"Clasificador Estadístico Paramétrico\", \"\\033[0m\", \"\\n\")\n",
    "\n",
    "clf = best_shrinkage_clf(X, y, k, [1.0, 0.8, 0.6, 0.5, 0.4, 0.2, 0.1, 0.0], [True, False])\n",
    "cbp = ClassifBayesianoParametrico(clf.share_covs, clf.shrinkage).fit(X,y)\n",
    "scoresCbp = cross_val_score(cbp, X, y, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCbp.mean(), scoresCbp.std() * 2), \"\\n\")\n",
    "mConfusionCbp = confusion_matrix(y, cbp.predict(X) , labels=np.unique(y))\n",
    "print(\"Matriz de confusión con clasificador bayesiano parametrico: \\n\", mConfusionCbp, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExclusionSplitter:\n",
    "    \"\"\"Esta clase nos permite usar GridSearchCV con la evaluación por exclusion.\"\"\"\n",
    "    def __init__(self, train_indices, test_indices):\n",
    "        self.train_indices = train_indices\n",
    "        self.test_indices = test_indices\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        return [(self.train_indices, self.test_indices)]\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolet Dataset (Isolated Letter Speech Recognition)\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de Isolet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-55c3c760452e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Loading MNIST from Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0misolet_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/isolet_subsets'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Loading MNIST from Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "isolet_dir = '/content/gdrive/My Drive/Colab Notebooks/isolet_subsets'\n",
    "\n",
    "# isolet_dir = 'isolet_subsets'\n",
    "\n",
    "train_indices = np.load(os.path.join(isolet_dir, 'isolet_train.npy'))\n",
    "test_indices = np.load(os.path.join(isolet_dir, 'isolet_test.npy'))\n",
    "\n",
    "# 1. Cargar los datos de la base de datos de entrenamiento\n",
    "X_all, y_all = fetch_openml('isolet', version=1, return_X_y=True, cache=True)\n",
    "y_all = pd.factorize(y_all)[0]\n",
    "X_train, y_train = X_all[train_indices], y_all[train_indices]\n",
    "X_test, y_test = X_all[test_indices], y_all[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Clasificador Euclídeo \u001b[0m \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-371e757481b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. Entrena el clasificador de la distancia eucídea empleando X_train e y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifEuclid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 3. Evalua el rendimiento en la base de datos de entrenamiento (X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dado el tamaño de la BD vamos emplear Exclusión como método de evaluación\n",
    "\n",
    "print(\"\\033[1m\", \"Clasificador Euclídeo\", \"\\033[0m\", \"\\n\")\n",
    "# 2. Entrena el clasificador de la distancia eucídea empleando X_train e y_train\n",
    "cde = ClassifEuclid()\n",
    "cde.fit(X_train,y_train)\n",
    "\n",
    "# 3. Evalua el rendimiento en la base de datos de entrenamiento (X_train, y_train)\n",
    "scoresCde = cde.num_aciertos(X_train, y_train)\n",
    "print(\"Entrenamiento C. distancia euclideo: \", scoresCde, \"\\n\")\n",
    "\n",
    "# 4. Evalua el rendimiento en la base de datos de test (X_test, y_test)\n",
    "scoresCde = cde.num_aciertos(X_test, y_test)\n",
    "print(\"Test C. distancia euclideo: \", scoresCde, \"\\n\")\n",
    "\n",
    "print(\"\\033[1m\", \"Clasificador Estadístico Paramétrico\", \"\\033[0m\", \"\\n\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Creamos un objeto spliter para dividir la base de datos en Train y test\n",
    "splitter = ExclusionSplitter(np.arange(0, len(X_train)), \n",
    "                             np.arange(len(X_train), len(X_train) + len(X_test)))\n",
    "\n",
    "# Decidimos los parámetros que vamos a pasar a GridSearchCV para hacer la búsqueda en rejilla\n",
    "# NOTA: Ten cuidado! Un gran número de celdas en la rejilla ralentizará la ejecución\n",
    "# Ejemplo de uso: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "params = {'shrinkage': [1.0, 0.5, 0.0], 'share_covs': [True]}\n",
    "\n",
    "# Definimos la búsqueda en rejilla. \n",
    "clf = GridSearchCV(ClassifBayesianoParametrico(), params, scoring='accuracy', cv=splitter, verbose=1)\n",
    "# Ejecutamos la búsqueda\n",
    "clf = clf.fit(X_all, y_all)\n",
    "# Seleccionamos el clasificador que mojores resultados ha obtenido \n",
    "best_clf = clf.best_estimator_\n",
    "# Mostramos los resultados\n",
    "print(\"Srinkage scores: \", clf.cv_results_['mean_test_score'])\n",
    "result_score_mean = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "result_score_std = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "print(\"\\tSelected shrinkage = {} Share covs = {} \\n\\tAccuracy: {:.3f} (+/- {:.3f})\".format(best_clf.shrinkage,\n",
    "                                                                                           best_clf.share_covs,\n",
    "                                                                                           result_score_mean,\n",
    "                                                                                           result_score_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Database (Modified National Institute of Standards and Technology database)\n",
    "MNIST es una base de datos de texto manuscrito, que se usa de forma clásica para entrenar sistemas de procesado de imágenes.\n",
    "\n",
    "Carga, entrenamiento, predicción y evaluación en la base de datos de MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-16e3da0413ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Loading MNIST from Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import idx2numpy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_mnsit_data(mnist_dir='MNIST', training_split=True):\n",
    "    \"\"\"\n",
    "    This function load the MNIST dataset from a disk directory.\n",
    "    :param mnist_dir: The directory where the MNIST Dataset is stored. The directory should contain the sub-directories:\n",
    "        - t10k-images-idx3-ubyte\n",
    "        - t10k-labels-idx1-ubyte\n",
    "        - train-images-idx3-ubyte\n",
    "        - train-labels-idx1-ubyte\n",
    "    :param training_split: True if we want to load the training examples and data, False for testing.\n",
    "    :return: A tuple with the training examples X in the first element and the y labels in the second one.\n",
    "    \"\"\"\n",
    "    assert type(mnist_dir) == str and type(training_split) == bool\n",
    "    assert os.path.isdir(mnist_dir), \"Error: The directory \\\"{}\\\" does not exists.\".format(mnist_dir)\n",
    "    file = os.path.join(mnist_dir, 'train-images-idx3-ubyte' if training_split else 't10k-images-idx3-ubyte', 'data')\n",
    "    X = idx2numpy.convert_from_file(file)\n",
    "    X = X.reshape(X.shape[0], -1).astype(float)\n",
    "    file = os.path.join(mnist_dir, 'train-labels-idx1-ubyte' if training_split else 't10k-labels-idx1-ubyte', 'data')\n",
    "    y = idx2numpy.convert_from_file(file)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Loading MNIST from Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "drive_dir = '/content/gdrive/My Drive/Colab Notebooks/MNIST'\n",
    "X_train, y_train = load_mnsit_data(drive_dir, training_split=True)\n",
    "X_test, y_test = load_mnsit_data(drive_dir, training_split=False)\n",
    "\n",
    "# X_train, y_train = load_mnsit_data(training_split=True)\n",
    "# X_test, y_test = load_mnsit_data(training_split=False)\n",
    "\n",
    "X_all = np.append(X_train, X_test, axis=0)\n",
    "y_all = np.append(y_train, y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Clasificador Euclídeo \u001b[0m \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-22286f680987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. Entrena el clasificador de la distancia eucídea empleando X_train e y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifEuclid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 3. Evalua el rendimiento en la base de datos de entrenamiento (X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dado el tamaño de la BD vamos emplear Exclusión como método de evaluación\n",
    "\n",
    "print(\"\\033[1m\", \"Clasificador Euclídeo\", \"\\033[0m\", \"\\n\")\n",
    "# 2. Entrena el clasificador de la distancia eucídea empleando X_train e y_train\n",
    "cde = ClassifEuclid()\n",
    "cde.fit(X_train,y_train)\n",
    "\n",
    "# 3. Evalua el rendimiento en la base de datos de entrenamiento (X_train, y_train)\n",
    "scoresCde = cde.num_aciertos(X_train, y_train)\n",
    "print(\"Entrenamiento C. distancia euclideo: \", scoresCde, \"\\n\")\n",
    "\n",
    "# 4. Evalua el rendimiento en la base de datos de test (X_test, y_test)\n",
    "scoresCde = cde.num_aciertos(X_test, y_test)\n",
    "print(\"Test C. distancia euclideo: \", scoresCde, \"\\n\")\n",
    "\n",
    "print(\"\\033[1m\", \"Clasificador Estadístico Paramétrico\", \"\\033[0m\", \"\\n\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Creamos un objeto spliter para dividir la base de datos en Train y test\n",
    "splitter = ExclusionSplitter(np.arange(0, len(X_train)), \n",
    "                             np.arange(len(X_train), len(X_train) + len(X_test)))\n",
    "\n",
    "# Decidimos los parámetros que vamos a pasar a GridSearchCV para hacer la búsqueda en rejilla\n",
    "# NOTA: Ten cuidado! Un gran número de celdas en la rejilla ralentizará la ejecución\n",
    "# Ejemplo de uso: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "params = {'shrinkage': [1.0, 0.5, 0.0], 'share_covs': [True]}\n",
    "\n",
    "# Definimos la búsqueda en rejilla. \n",
    "clf = GridSearchCV(ClassifBayesianoParametrico(), params, scoring='accuracy', cv=splitter, verbose=1)\n",
    "# Ejecutamos la búsqueda\n",
    "clf = clf.fit(X_all, y_all)\n",
    "# Seleccionamos el clasificador que mojores resultados ha obtenido \n",
    "best_clf = clf.best_estimator_\n",
    "# Mostramos los resultados\n",
    "print(\"Srinkage scores: \", clf.cv_results_['mean_test_score'])\n",
    "result_score_mean = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "result_score_std = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "print(\"\\tSelected shrinkage = {} Share covs = {} \\n\\tAccuracy: {:.3f} (+/- {:.3f})\".format(best_clf.shrinkage,\n",
    "                                                                                           best_clf.share_covs,\n",
    "                                                                                           result_score_mean,\n",
    "                                                                                           result_score_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados de los experimentos. Pon el mejor de los resultados:\n",
    "\n",
    "| Base de datos | Accuracy | Desviación Típica |\n",
    "| --- | --- | --- |\n",
    "| Iris   |||\n",
    "| Wine   | ||\n",
    "| Cancer |||\n",
    "| MNIST  | ||\n",
    "| Isolet |||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
